Program starts:  14:33:38
/var/spool/slurm/job3645411/slurm_script: line 15: conda_initialize: command not found
Running with n=3
/gpfs/bwfor/home/ma/ma_ma/ma_kyupark/micromamba/envs/is809/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]
/gpfs/bwfor/home/ma/ma_ma/ma_kyupark/micromamba/envs/is809/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/gpfs/bwfor/home/ma/ma_ma/ma_kyupark/is809/codes/src/utils/preprocess.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  selected_rows = pd.concat([selected_rows, semifinal])
/gpfs/bwfor/home/ma/ma_ma/ma_kyupark/micromamba/envs/is809/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /home/ma/ma_ma/ma_kyupark/.cache/huggingface/token
Login successful
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.35s/it]
  0%|          | 0/5 [00:00<?, ?it/s]/gpfs/bwfor/home/ma/ma_ma/ma_kyupark/micromamba/envs/is809/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
 20%|██        | 1/5 [00:01<00:06,  1.54s/it]/gpfs/bwfor/home/ma/ma_ma/ma_kyupark/micromamba/envs/is809/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
 40%|████      | 2/5 [00:02<00:03,  1.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
 60%|██████    | 3/5 [00:02<00:01,  1.11it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
 80%|████████  | 4/5 [00:03<00:00,  1.19it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
100%|██████████| 5/5 [00:04<00:00,  1.12it/s]100%|██████████| 5/5 [00:04<00:00,  1.06it/s]
Traceback (most recent call last):
  File "/gpfs/bwfor/home/ma/ma_ma/ma_kyupark/is809/codes/run_reasonedfewshot.py", line 89, in <module>
    main(args.filename, args.n, args.labeled_file, args.output_file)
  File "/gpfs/bwfor/home/ma/ma_ma/ma_kyupark/is809/codes/run_reasonedfewshot.py", line 74, in main
    perf.to_pickle(output_file)
UnboundLocalError: local variable 'perf' referenced before assignment
Completed run with n=3
Completed: 14:34:25
